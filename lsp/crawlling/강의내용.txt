크롤링 : 해당 사이트안에 모든내용을 무작위로 방대하게 데이터 수집 , 일정한 시간 간격으로
스크래핑 : 타겟이되는 데이터 파싱 , 재생산 

크롬 개발자 도구
블럭 씌우고 마우스 오른쪽 클릭
 Elements tab = css selector
network tab - http 처리과정 

robots.txt
상태 유무인지 , js로 그려주는지 -> 샐래늄 사용 

서버 부하를 고려해야함 , 저작권 문제

대형 사이트들은 data api가 따로있어서 서버부하없이 더 체계적으로 데이터 수집가능 

되도록이면 data api사용 , 아니면 robotx.txt를 사용 